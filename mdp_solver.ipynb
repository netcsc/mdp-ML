{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def value_iteration(env, theta=0.0001, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than delta for all states.\n",
    "        gamma: Gamma discount factor.\n",
    "    Returns:\n",
    "        A tuple (policy, V, iterations) of the optimal policy, the optimal value function, and iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + gamma * V[next_state])\n",
    "        return A\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            A = one_step_lookahead(state, V)  \n",
    "            best_act_value = np.max(A)\n",
    "            delta = max(delta, np.abs(best_act_value - V[state]))\n",
    "            V[state] = best_act_value  # update value to best action value\n",
    "        if delta < theta:  # if delta improvement is less than threshold\n",
    "            break\n",
    "            \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for state in range(env.nS): \n",
    "        A = one_step_lookahead(state, V)\n",
    "        best_action = np.argmax(A)\n",
    "        policy[state][best_action] = 1.0\n",
    "        \n",
    "    return policy, V, iterations\n",
    "\n",
    "def policy_evaluation(policy, env, theta=0.00001, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        gamma: Gamma discount factor.\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.nS):\n",
    "            val = 0  # initiate value as 0\n",
    "            for action, act_prob in enumerate(policy[state]):  # for all actions/action probabilities\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    val += act_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(val - V[state]))\n",
    "            V[state] = val\n",
    "        if delta < theta:  # if delta improvement is less than threshold\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "\n",
    "def policy_iteration(env, policy_eval_function=policy_evaluation, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_function: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, gamma.\n",
    "        discount_factor: gamma discount factor.\n",
    "    Returns:\n",
    "        A tuple (policy, V).\n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + gamma * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA # Start with a random policy\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        curr_pol_value = policy_eval_function(policy, env, gamma)  # eval current policy\n",
    "        policy_stable = True  # Check if policy did improve\n",
    "        for state in range(env.nS): \n",
    "            chosen_act = np.argmax(policy[state])  # best action base on current policy\n",
    "            act_values = one_step_lookahead(state, curr_pol_value)\n",
    "            best_act = np.argmax(act_values)  # find best action\n",
    "            if chosen_act != best_act:\n",
    "                policy_stable = False  # Greedily find best action\n",
    "            policy[state] = np.eye(env.nA)[best_act]  # update\n",
    "        if policy_stable:\n",
    "            return policy, curr_pol_value, iterations\n",
    "\n",
    "    return policy, np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Value Iteration: 7.247082096999975s in 610 iters\n"
    }
   ],
   "source": [
    "import gym\n",
    "from timeit import default_timer as timer\n",
    "env_name  = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "start = timer()\n",
    "policy, V, iterations = value_iteration(env, gamma=0.99)\n",
    "end = timer()\n",
    "print(\"Value Iteration: {}s in {} iters\".format(end - start, iterations))\n",
    "print(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}